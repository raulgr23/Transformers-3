{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Modelo ViT para la clasificación de imágenes**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-09T16:23:00.474524Z","iopub.execute_input":"2022-06-09T16:23:00.474978Z","iopub.status.idle":"2022-06-09T16:23:21.850258Z","shell.execute_reply.started":"2022-06-09T16:23:00.474885Z","shell.execute_reply":"2022-06-09T16:23:21.849196Z"}}},{"cell_type":"markdown","source":"Descargamos algunas librerias necesarias para la implementación","metadata":{}},{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:53:50.414653Z","iopub.execute_input":"2022-06-13T17:53:50.415211Z","iopub.status.idle":"2022-06-13T17:54:03.959464Z","shell.execute_reply.started":"2022-06-13T17:53:50.415104Z","shell.execute_reply":"2022-06-13T17:54:03.958185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importamos las librerías","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nfrom torch import nn\nfrom torch import Tensor\nfrom PIL import Image\nfrom torchvision.transforms import Compose, Resize, ToTensor\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\n\nfrom einops import rearrange, reduce, repeat\nfrom einops.layers.torch import Rearrange, Reduce","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:54:03.962451Z","iopub.execute_input":"2022-06-13T17:54:03.963019Z","iopub.status.idle":"2022-06-13T17:54:04.957573Z","shell.execute_reply.started":"2022-06-13T17:54:03.962954Z","shell.execute_reply":"2022-06-13T17:54:04.956486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cargamos los datos","metadata":{}},{"cell_type":"code","source":"transforms = Compose([Resize((224, 224)), ToTensor()])\n\ntraining_data = ImageFolder(root=\"../input/iais22-birds/birds/birds\", transform = transforms)\ntest_data = ImageFolder(root=\"../input/iais22-birds/submission_test\", transform = transforms)\n\ntrain_set, test_set = random_split(training_data, (int(len(training_data) * 0.7) + 1, int(len(training_data) * 0.3)))\n\ntrain_dataloader = DataLoader(train_set, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_set, batch_size=64, shuffle=True)\n\nprint(f\"Training data size: {train_set}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:54:04.959208Z","iopub.execute_input":"2022-06-13T17:54:04.960193Z","iopub.status.idle":"2022-06-13T17:54:20.947689Z","shell.execute_reply.started":"2022-06-13T17:54:04.960145Z","shell.execute_reply":"2022-06-13T17:54:20.946477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creamos un diccionario que mapea id de la clase con su nombre","metadata":{}},{"cell_type":"code","source":"clases_list = training_data.classes\nclases = {}\ncont = 0\nfor i in clases_list:\n    clases[cont] = i\n    cont+=1\nprint(clases)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:54:20.950717Z","iopub.execute_input":"2022-06-13T17:54:20.951484Z","iopub.status.idle":"2022-06-13T17:54:20.961542Z","shell.execute_reply.started":"2022-06-13T17:54:20.951435Z","shell.execute_reply":"2022-06-13T17:54:20.960406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comprobamoos que tanto las imagenes como las targets se han guardado correctamente","metadata":{}},{"cell_type":"code","source":"train_features, train_labels = training_data.__getitem__(0)\nprint(f\"Tamaño de cada imagen: {train_features.size()}\")\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(clases[label])\n    plt.axis(\"off\")\n    plt.imshow(img[1][:][:], cmap=\"gray\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:54:20.962792Z","iopub.execute_input":"2022-06-13T17:54:20.963235Z","iopub.status.idle":"2022-06-13T17:54:21.643748Z","shell.execute_reply.started":"2022-06-13T17:54:20.963165Z","shell.execute_reply":"2022-06-13T17:54:21.642858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comprobamos que los DataLoaders funcionan correctamente","metadata":{}},{"cell_type":"code","source":"for X, y in train_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:54:21.64495Z","iopub.execute_input":"2022-06-13T17:54:21.645403Z","iopub.status.idle":"2022-06-13T17:54:22.146011Z","shell.execute_reply.started":"2022-06-13T17:54:21.645363Z","shell.execute_reply":"2022-06-13T17:54:22.14475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comprobamos si está disponible la GPU","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:54:22.147475Z","iopub.execute_input":"2022-06-13T17:54:22.148016Z","iopub.status.idle":"2022-06-13T17:54:22.227247Z","shell.execute_reply.started":"2022-06-13T17:54:22.147954Z","shell.execute_reply":"2022-06-13T17:54:22.225956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comenzamos a construir la arquitectura ViT. Usaremos una técnica modular de contruir la estructura poco a poco con distintas clases que heredan de nn.Module(). Empezamos construyendo el PachEmbedding capaz de trocear la imagen en imagenes de 16x16 y asignarles la posicion (parametro que se aprende) siguiendo el paper *AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE*","metadata":{}},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, img_size: int = 224, channels: int = 3, section: int = 16, output_net: int = 768):\n        super().__init__()\n        self.patch_size = section\n        self.positions = nn.Parameter(torch.randn((img_size // section) **2, output_net))\n        self.pos_drop = nn.Dropout(p=0.)\n        self.network = nn.Sequential(\n            Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=section, s2=section),\n            nn.Linear(section * section * channels, output_net)\n        )\n                \n    def forward(self, images):\n        images = self.network(images)\n        #print(self.positions.size())\n        #print(images + self.positions)\n        #print(images.size())\n        images = self.pos_drop(images + self.positions)\n        return images","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:54:22.229219Z","iopub.execute_input":"2022-06-13T17:54:22.229987Z","iopub.status.idle":"2022-06-13T17:54:22.242534Z","shell.execute_reply.started":"2022-06-13T17:54:22.229936Z","shell.execute_reply":"2022-06-13T17:54:22.24098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para contruir el transformer construiremos sus partes paso a paso. Construimos el módulo Multihead Attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, output_net: int = 512, num_heads: int = 8, dropout: float = 0):\n        super().__init__()\n        self.output_net = output_net\n        self.num_heads = num_heads\n        self.keys = nn.Linear(output_net, output_net)\n        self.queries = nn.Linear(output_net, output_net)\n        self.values = nn.Linear(output_net, output_net)\n        self.att_drop = nn.Dropout(dropout)\n        self.network = nn.Linear(output_net, output_net)\n        \n    def forward(self, images):\n        #print(images.size())\n        queries = rearrange(self.queries(images), \"b n (h d) -> b h n d\", h=self.num_heads)\n        keys = rearrange(self.keys(images), \"b n (h d) -> b h n d\", h=self.num_heads)\n        values  = rearrange(self.values(images), \"b n (h d) -> b h n d\", h=self.num_heads)\n        \n        attention = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n            \n        scaling = self.output_net ** (1/2)\n        att = F.softmax(attention, dim=-1) / scaling\n        att = self.att_drop(att)\n        \n        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n        out = self.network(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:54:22.244231Z","iopub.execute_input":"2022-06-13T17:54:22.245222Z","iopub.status.idle":"2022-06-13T17:54:22.259497Z","shell.execute_reply.started":"2022-06-13T17:54:22.245175Z","shell.execute_reply":"2022-06-13T17:54:22.258304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definimos el componente del Transformer FeedForward que alimenta a la red hacia adelante","metadata":{}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, output: int = 768):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(output, 4 * output),\n            nn.GELU(),\n            nn.Dropout(0.),\n            nn.Linear(output * 4 , output)\n        )\n        \n    def forward(self, images):\n        images=self.network(images)\n        return images\n","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:54:22.264249Z","iopub.execute_input":"2022-06-13T17:54:22.264988Z","iopub.status.idle":"2022-06-13T17:54:22.286907Z","shell.execute_reply.started":"2022-06-13T17:54:22.264775Z","shell.execute_reply":"2022-06-13T17:54:22.285917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creamos las conexiones residuales","metadata":{}},{"cell_type":"code","source":"class ResidualConection(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n        \n    def forward(self, images):\n        res = images\n        #print(images.size())\n        images = self.fn(images)\n        images += res\n        return images","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:54:22.288441Z","iopub.execute_input":"2022-06-13T17:54:22.289033Z","iopub.status.idle":"2022-06-13T17:54:22.308535Z","shell.execute_reply.started":"2022-06-13T17:54:22.288989Z","shell.execute_reply":"2022-06-13T17:54:22.307231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finalmente construimos el Transformer Encoder","metadata":{}},{"cell_type":"code","source":"output = 768\ncapa0=ResidualConection(\n                nn.Sequential(\n                    nn.LayerNorm(output),\n                    MultiHeadAttention(output),\n                    nn.Dropout(0.)\n                )\n            )\ncapa1=ResidualConection(\n                nn.Sequential(\n                    nn.LayerNorm(output),\n                    FeedForward(output),\n                    nn.Dropout(0.)\n                ),\n            )\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, output: int = 768):\n        super().__init__()\n        self.network = nn.Sequential(capa0, capa1)\n        \n    def forward(self, images):\n        images = self.network(images)\n        return images\n","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:54:22.310303Z","iopub.execute_input":"2022-06-13T17:54:22.310939Z","iopub.status.idle":"2022-06-13T17:54:22.386548Z","shell.execute_reply.started":"2022-06-13T17:54:22.310893Z","shell.execute_reply":"2022-06-13T17:54:22.385484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definimos el módulo Transformer como composicion de varios TransformerEncorders siguiendo el paper *All you need is Attention*","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, output: int = 768):\n        super().__init__()\n        self.network = nn.Sequential(TransformerEncoder(),\n                                     TransformerEncoder(),\n                                     TransformerEncoder(),\n                                     TransformerEncoder(),\n                                     TransformerEncoder(),\n                                     TransformerEncoder(),\n                                     TransformerEncoder(),\n                                     TransformerEncoder(),\n                                    )\n        \n    def forward(self, images):\n        images= self.network(images)\n        return images","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:54:22.389233Z","iopub.execute_input":"2022-06-13T17:54:22.389903Z","iopub.status.idle":"2022-06-13T17:54:22.398408Z","shell.execute_reply.started":"2022-06-13T17:54:22.389855Z","shell.execute_reply":"2022-06-13T17:54:22.3972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creamos el módulo MLPHead capaz de dar la clasificación de la imagen","metadata":{}},{"cell_type":"code","source":"class MLPHead(nn.Module):\n    def __init__(self, output: int = 768, n_classes: int = 400):\n        super().__init__()\n        self.network = nn.Sequential(\n            Reduce('b n e -> b e', reduction='mean'),\n            nn.LayerNorm(output), \n            nn.Linear(output, n_classes))\n        \n    def forward(self, images):\n        images=self.network(images)\n        return images","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:54:22.399808Z","iopub.execute_input":"2022-06-13T17:54:22.400984Z","iopub.status.idle":"2022-06-13T17:54:22.411722Z","shell.execute_reply.started":"2022-06-13T17:54:22.400936Z","shell.execute_reply":"2022-06-13T17:54:22.410512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definimos el módulo ViT como composicion de los anteriores modulos creados siguiendo el paper","metadata":{}},{"cell_type":"code","source":"class ViT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            PatchEmbedding(),\n            Transformer(),\n            MLPHead()\n        )\n        \n    def forward(self, images):\n        images = self.network(images)\n        return images","metadata":{"execution":{"iopub.status.busy":"2022-06-13T17:54:22.413477Z","iopub.execute_input":"2022-06-13T17:54:22.414215Z","iopub.status.idle":"2022-06-13T17:54:22.423405Z","shell.execute_reply.started":"2022-06-13T17:54:22.414167Z","shell.execute_reply":"2022-06-13T17:54:22.422264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definimos el modelo, seleccionamos la GPU para el entrenamiento y la funcion de perdida y de optimizacion","metadata":{}},{"cell_type":"code","source":"#model = ViT()\nmodel = torch.load(\"./model45.pth\")\nmodel.to(device)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n#print(model)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T19:02:16.566145Z","iopub.execute_input":"2022-06-13T19:02:16.566551Z","iopub.status.idle":"2022-06-13T19:02:16.602912Z","shell.execute_reply.started":"2022-06-13T19:02:16.566519Z","shell.execute_reply":"2022-06-13T19:02:16.601837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Definimos las funciones de entrenamiento y testeo del modelo","metadata":{}},{"cell_type":"code","source":"def train(train_dataloader, model, loss_fn, optimizer):\n    size = len(train_dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(train_dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n            \ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n            \n","metadata":{"execution":{"iopub.status.busy":"2022-06-13T19:02:17.880926Z","iopub.execute_input":"2022-06-13T19:02:17.88146Z","iopub.status.idle":"2022-06-13T19:02:17.893094Z","shell.execute_reply.started":"2022-06-13T19:02:17.88142Z","shell.execute_reply":"2022-06-13T19:02:17.891692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Entrenamos y guardamos al modelo con distintos numero de epocas","metadata":{}},{"cell_type":"code","source":"epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Done!\")\ntorch.save(model, \"model50.pth\")\nprint(\"Model saved\")","metadata":{"execution":{"iopub.status.busy":"2022-06-13T19:02:19.482599Z","iopub.execute_input":"2022-06-13T19:02:19.483085Z","iopub.status.idle":"2022-06-13T19:48:29.287652Z","shell.execute_reply.started":"2022-06-13T19:02:19.483044Z","shell.execute_reply":"2022-06-13T19:48:29.286344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}